# -*- coding: utf-8 -*-
"""AI Student-tutor Recommended System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DOEBIX_RFwnDO7hoQbze_0BS9r5tLcXJ

# **Welcome to our Tutor Recommendation System project!**

This Google Colab notebook implements a hybrid AI-based recommender for a ‚ÄúTutor Uberization‚Äù platform that connects students with the most suitable tutors based on multiple personalized criteria. The model combines:

‚úì Content-based filtering using educational and preference attributes  
‚úì Collaborative filtering using Matrix Factorization (SVD) trained on rating interactions  
‚úì A hybrid scoring system to balance relevance + personalization  

**This notebook includes:**

1. Dataset loading and preprocessing  
2. Feature engineering and soft rule-based matching  
3. Collaborative filtering model training using Surprise SVD  
4. Hybrid recommendation generation for each student  
5. Evaluation using Precision@K, Recall@K, and NDCG@K  
6. A simple user interface for student input testing  

This project is part of an academic study exploring how AI improves educational accessibility in Saudi Arabia by automating tutor-student matching.

# Load Datasets from Google Drive
In this step, we mount Google Drive to access the stored datasets.
We load the core data sources required for the recommendation system:

* "Student Profile dataset"

* "Tutor Course dataset"

* "Interaction history dataset"

* "Curriculum Subject dataset"


Displaying the first few rows ensures the files loaded correctly and the structure matches expectations.
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

BASE_PATH = "/content/drive/MyDrive/tutor_uberization_datasets"

student_df     = pd.read_csv(f"{BASE_PATH}/StudentProfile.csv")
interaction_df = pd.read_csv(f"{BASE_PATH}/InteractionDataset.csv")
tutors_df      = pd.read_csv(f"{BASE_PATH}/TutorCourse.csv")
subjects_df    = pd.read_csv(f"{BASE_PATH}/CurriculumSubject.csv")

display(student_df.head())
display(interaction_df.head())
display(tutors_df.head())
display(subjects_df.head())

"""#Education Type Reduction: From 19 ‚ûù 5 Groups

Originally, the dataset contained 19 unique education curriculums, such as:
Saudi MOE, IB, IGCSE, AP, A-Level, Australian Curriculum, French Curriculum, etc.

We replaced 19 unique education curriculums with 5 major groups:

| Final Groups  | Reason                                    |
| ------------- | ----------------------------------------- |
| MOE           | Most common in KSA                        |
| British       | IGCSE, A-Level, O-Level‚Ä¶ shared structure |
| American      | Diploma, AP, SAT‚Ä¶ similar grading         |
| International | IB, Australian, French                    |
| Other         | Minor representation, avoids sparsity     |


‚û° This improved training data density ‚Üë, and accuracy ‚Üë
"""

# === AFTER we loaded student_df and tutors_df ===

edu_map = {
    "Saudi National (MOE)": "MOE",
    "Religious / Sharia Education": "MOE",

    "IGCSE (Cambridge / Edexcel)": "British",
    "O-Level (British Council)": "British",
    "A-Level (Advanced Level)": "British",
    "Pakistani Curriculum (FBISE / Cambridge)": "British",
    "Jordanian Curriculum (Tawjihi)": "British",
    "Egyptian Curriculum (Thanaweya Amma / EST)": "British",  # or British-like

    "American Diploma (Common Core)": "American",
    "American (SAT)": "American",
    "American (AP - Advanced Placement)": "American",
    "Canadian Curriculum": "American",

    "International Baccalaureate (IB)": "International",
    "Australian Curriculum": "International",
    "French Curriculum": "International",
}

# Tutors
tutors_df["education_type_group"] = tutors_df["education_type"].map(edu_map).fillna("Other")

# Students
student_df["education_type_group"] = student_df["education_type"].map(edu_map).fillna("Other")

print(student_df[["education_type", "education_type_group"]].head())
print(tutors_df[["education_type", "education_type_group"]].head())

print(student_df['education_type'].unique())

"""# Data Cleaning & Standardization
This cell performs essential preprocessing to ensure data consistency:

* **Missing values in** `special_needs` **are filled with** `"none"` **to avoid errors during filtering.**

* **Text fields such as** `subject_name` **and** `course_name` **are converted to lowercase and stripped of whitespace to ensure accurate matching between students and tutors.**

This step improves data quality and ensures rule-based filtering behaves correctly.
"""

# Handle missing values
student_df['special_needs'].fillna('none', inplace=True)

# Standardize key text columns
for df in [student_df, tutors_df]:
    df['subject_name'] = df['subject_name'].str.lower().str.strip()
    df['course_name'] = df['course_name'].str.lower().str.strip()

display(student_df.head())
display(tutors_df.head())

student_df.info()

"""# Baseline Rule-Based Recommendation

These cells implement a **baseline filtering logic** using strict matching rules:

* **"subject_name"**

* **"course_name"**

* **"education_type"**

* **"gender preference and tutor gender restrictions"**

The system retrieves tutors who meet all required criteria and returns the top-5.
This serves as a **baseline model** for comparison later against the hybrid recommender.
"""

def recommend_rule_based(student_id):
    s = student_df[student_df['student_id']==student_id].iloc[0]

    df = tutors_df.copy()

    df = df[df['subject_name'] == s['subject_name']]
    df = df[df['course_name'] == s['course_name']]
    df = df[df['education_type'] == s['education_type']]

    # Gender preference & restriction matching
    preferred = str(s.get('preferred_tutor_gender','Any')).lower()
    student_gender = str(s.get('gender', '')).lower()

    if preferred != 'any':
        df = df[df['tutor_gender'].str.lower() == preferred]

    df = df[
        df['teaches_gender'].str.lower().isin(
            ['mixed', 'both', student_gender + ' only']
        )
    ]

    return df[['tutor_id','subject_name','course_name','education_type',
            'tutor_gender','teaches_gender','average_rating_overall']].head(5)

for sid in student_df['student_id'].head(3):
    print(f"\nTop tutors for student {sid}")
    display(recommend_rule_based(sid))

"""# Install Required Packages

The Surprise library is used for the Matrix Factorization model (SVD).
It requires NumPy version < 2.0.

These commands downgrade NumPy and install a compatible version of scikit-surprise.
"""

!pip install numpy==1.26.4
!pip install scikit-surprise==1.1.3

"""# Import Surprise Libraries
This cell imports necessary Surprise components:

* "`Dataset` **and** `Reader` **to prepare interaction data**"

* "`SVD` **for matrix factorization recommendations**"

* "`train_test_split` **to evaluate the model on unseen data**"

"""

from surprise import Dataset, Reader, SVD
from surprise.model_selection import train_test_split
import numpy as np

"""## Build the Interactions Dataset
The recommender model only requires:

* "`student_id`"

* "`tutor_id`"

* "`rating`"

This step extracts those fields, removes rows with missing ratings, and ensures the rating column is numeric.

This produces a clean interaction matrix for model training.
"""

# ============================================
# 1) Ensure grouped education types exist
# ============================================
# edu_map MUST already be defined earlier.
# If not, define it again here:

edu_map = {
    "Saudi National (MOE)": "MOE",
    "Religious / Sharia Education": "MOE",
    "Quran / Tahfeeth Schools": "MOE",
    "IGCSE": "British",
    "GCSE": "British",
    "AS Level": "British",
    "A Level": "British",
    "British Curriculum": "British",
    "American Diploma": "American",
    "American Curriculum": "American",
    "IB": "International",
    "International Baccalaureate (IB)": "International",
    "Australian Curriculum": "International",
    "French Curriculum": "International",
}

# If column not there for some reason, create it
if 'education_type_group' not in student_df.columns:
    student_df["education_type_group"] = student_df["education_type"].map(edu_map).fillna("Other")

if 'education_type_group' not in tutors_df.columns:
    tutors_df["education_type_group"] = tutors_df["education_type"].map(edu_map).fillna("Other")

print("Student education_type_group value counts:")
print(student_df["education_type_group"].value_counts())
print("\nTutor education_type_group value counts:")
print(tutors_df["education_type_group"].value_counts())


# ============================================
# 2) Build interactions with SOFT group compatibility
# ============================================
# interactions = only (student_id, tutor_id, rating) needed for SVD
interactions = interaction_df[['student_id', 'tutor_id', 'rating']].copy()

# Remove missing ratings if any
interactions = interactions.dropna(subset=['rating'])

# Ensure rating is numeric
interactions['rating'] = interactions['rating'].astype(float)

# Attach education groups
interactions = interactions.merge(
    student_df[['student_id', 'education_type_group']],
    on='student_id', how='left'
).rename(columns={'education_type_group': 'student_ed_group'})

interactions = interactions.merge(
    tutors_df[['tutor_id', 'education_type_group']],
    on='tutor_id', how='left'
).rename(columns={'education_type_group': 'tutor_ed_group'})


# -------- Soft compatibility function --------
def compatible_groups(stu, tut):
    """
    Soften the constraint:
      - Exact match is best
      - 'Other' is compatible with anything
      - International <-> (American/British) treated as compatible
    """
    if pd.isna(stu) or pd.isna(tut):
        return False

    if stu == tut:
        return True

    if stu == "Other" or tut == "Other":
        return True

    if stu == "International" and tut in ["American", "British"]:
        return True
    if tut == "International" and stu in ["American", "British"]:
        return True

    return False


# Apply soft compatibility to keep only "reasonable" pairs
mask = interactions.apply(
    lambda row: compatible_groups(row['student_ed_group'], row['tutor_ed_group']),
    axis=1
)

interactions = interactions[mask].copy()

print("\nInteraction dataset AFTER SOFT education group compatibility filter:")
display(interactions.head())
print("Remaining training rows:", len(interactions))

# Final dataframe used by Surprise: ONLY the columns it needs
interactions_for_surprise = interactions[['student_id', 'tutor_id', 'rating']].copy()

from surprise import Dataset, Reader

reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(interaction_df[['student_id','tutor_id','rating']], reader)

"""# Train/Test Split + SVD Model Training

The rating scale is defined as `[1,5]`.
Surprise converts the interaction DataFrame into an internal dataset format for SVD.

The data is split:

* **"80% for training ‚Üí the model learns hidden preferences"**

* **"20% for testing ‚Üí measures performance on unseen interactions"**

The SVD (Singular Value Decomposition) model is trained to predict how much a student would like a tutor they have not interacted with yet.
"""

from surprise.model_selection import train_test_split

trainset, testset = train_test_split(data, test_size=0.2, random_state=42)
model = SVD()
model.fit(trainset)

# Generate test predictions
predictions = model.test(testset)

print("Predictions generated:", len(predictions))

"""# Generate Soft Rule-Based Tutor Candidates
Hard filtering can lead to zero recommendations.
Instead, this function applies soft matching where tutors receive points if they align with:

* **"Subject & curriculum"**

* **"Course match"**

* **"Grade level compatibility**

* **"Session type & group preferences"**

* **"City proximity"**

* **"Gender compatibility"**

* **"Support for special needs"**

* **"Performance match"**

* **"Experience & rating strengths"**

This always provides a **candidate pool** of reasonable tutors for personalization.
"""

def get_soft_candidates_for_student(s, tutors_df):
    """
    Soft filtering: use rules to score tutors instead of hard rejecting.
    Now:
      - Always start from same subject (if possible)
      - Use strong weights for subject, course, grade, and education_type_group
      - Still consider session type, group type, city, gender, special needs,
        performance, experience, and rating.
    """

    df = tutors_df.copy()

    # 1) Start with same subject if possible (critical for accuracy)
    if pd.notna(s.get('subject_name')):
        df_subj = df[df['subject_name'] == s['subject_name']]
        if not df_subj.empty:
            df = df_subj  # prefer same subject

    def match_score(row):
        score = 0

        # --- SUBJECT (very important) ---
        if row.get('subject_name') == s.get('subject_name'):
            score += 5

        # --- COURSE NAME ---
        if pd.notna(s.get('course_name')) and pd.notna(row.get('course_name')):
            if row['course_name'] == s['course_name']:
                score += 4
            else:
                sc = s['course_name'].lower()
                tc = row['course_name'].lower()
                if any(k in sc and k in tc for k in
                       ['math', 'algebra', 'geometry', 'physics',
                        'chem', 'biology', 'english', 'arabic']):
                    score += 2

        # --- GRADE LEVEL ---
        try:
            g_student = int(s.get('grade_level'))
            g_tutor = int(row.get('grade_level'))
            if g_student == g_tutor:
                score += 4
            elif abs(g_student - g_tutor) == 1:
                score += 2
            else:
                score -= 2
        except Exception:
            pass

        # --- EDUCATION TYPE GROUP (new!) ---
        stu_group = s.get('education_type_group')
        tut_group = row.get('education_type_group')
        if pd.notna(stu_group) and pd.notna(tut_group):
            if stu_group == tut_group:
                score += 3
            else:
                score -= 1  # small penalty if different group

        # (keep your old raw education_type logic if you want)
        # stu_edu = s.get('education_type')
        # tut_edu = row.get('education_type')
        # ...

        # --- SESSION TYPE ---
        pref_session = str(s.get('preferred_session_type', '')).lower()
        t_session = str(row.get('session_type', '')).lower()
        if pref_session and t_session:
            if pref_session == t_session:
                score += 2
            elif pref_session in t_session or t_session in ['hybrid', 'both']:
                score += 1

        # --- GROUP TYPE ---
        pref_group = str(s.get('preferred_session_group_type', '')).lower()
        t_group = str(row.get('group_or_private', '')).lower()
        if pref_group and t_group and pref_group == t_group:
            score += 1

        # --- CITY for F2F/Hybrid ---
        student_city = s.get('location_city')
        t_city = row.get('location_city')
        if pref_session in ['face-to-face', 'hybrid']:
            if student_city and t_city:
                if str(student_city).lower() == str(t_city).lower():
                    score += 2
                else:
                    score -= 2

        # --- GENDER PREFERENCE + RESTRICTIONS ---
        student_gender = str(s.get('gender', '')).lower()
        pref_tutor_gender = str(s.get('preferred_tutor_gender', 'any')).lower()
        t_gender = str(row.get('tutor_gender', '')).lower()
        teaches_gender = str(row.get('teaches_gender', '')).lower()

        if pref_tutor_gender != 'any' and t_gender == pref_tutor_gender:
            score += 2

        if student_gender:
            if 'mixed' in teaches_gender or 'both' in teaches_gender:
                score += 1
            elif 'female' in teaches_gender and student_gender == 'female':
                score += 1
            elif 'male' in teaches_gender and student_gender == 'male':
                score += 1
            else:
                score -= 4

        # --- SPECIAL NEEDS ---
        need = str(s.get('special_needs', 'none')).lower()
        handles = str(row.get('handles_special_needs', '')).lower()
        if need not in ['none', '', 'nan']:
            if need in handles:
                score += 3
            else:
                score -= 3

        # --- PERFORMANCE TIER ---
        perf = str(s.get('performance_tier', '')).lower()
        perf_focus = str(row.get('performance_focus', '')).lower()
        if perf and perf_focus and perf in perf_focus:
            score += 2

        # --- EXPERIENCE ---
        try:
            years = float(row.get('experience_years', 0))
            if years >= 10:
                score += 2
            elif years >= 5:
                score += 1
        except Exception:
            pass

      # --- AVERAGE RATING BOOST (High Impact) ---
        try:
              r = float(row['average_rating_overall'])
              if   r >= 4.7: score += 8
              elif r >= 4.3: score += 5
              elif r >= 4.0: score += 2
              elif r < 3.5: score -= 5
        except:
              pass


        return score

    df = df.copy()
    df['match_score'] = df.apply(match_score, axis=1)

    if (df['match_score'] > 0).any():
        df = df[df['match_score'] > 0]

    return df

"""# Hybrid Recommendation System

This hybrid method combines:

‚û§ **Rule-based soft scores**

‚û§ **SVD-predicted rating score**

Steps:

**1.**   **"Select candidate tutors using the soft rule filter"**

**2.**   **"Predict estimated student rating for each tutor"**

**3. Sort by:**

* **"Match Score (filter compatibility)"**

* **"Predicted Rating (personal preference)"**

This creates personalized and intelligent top-N tutor recommendations.
"""

def recommend_tutors_hybrid(student_id, top_n=None):
    """
    Hybrid recommender:
      1) Strong soft-rule scoring (match_score)
      2) SVD predicted rating
      3) Combined final_score = 0.25 * normalized_match_score + 0.75 * normalized_pred
         (SVD now has more weight to improve precision)

    If top_n is None ‚Üí return ALL unique tutors sorted by final_score.
    If top_n is an integer ‚Üí return only the top_n tutors.
    """
    # --- Get student row ---
    s_rows = student_df[student_df['student_id'] == student_id]
    if s_rows.empty:
        print(f"No student found with id {student_id}")
        return pd.DataFrame()
    s = s_rows.iloc[0]

    # --- 1) Rule-based candidates ---
    candidates = get_soft_candidates_for_student(s, tutors_df)
    if candidates.empty:
        print("‚ö† No tutors at all (tutors_df is empty or no matches).")
        return pd.DataFrame()

    # --- 2) Predict ratings with SVD for each candidate tutor ---
    scored = []
    for _, t in candidates.iterrows():
        tutor_id = t['tutor_id']
        pred = model.predict(str(student_id), str(tutor_id))
        scored.append((tutor_id, pred.est))

    scores_df = pd.DataFrame(scored, columns=['tutor_id', 'predicted_rating'])

    # Merge rule-based candidates with predicted ratings
    result = candidates.merge(scores_df, on='tutor_id')

    # --- 2.5) Hard filter: drop tutors with low predicted rating (improves precision) ---
    PRED_CUTOFF = 3.8  # try 3.8 or 4.0 and see impact on precision/recall
    filtered = result[result['predicted_rating'] >= PRED_CUTOFF]

    # If filtering removes everything, fall back to unfiltered result
    if not filtered.empty:
        result = filtered

    # --- 3) Normalize match_score and predictions ---
    ms = result['match_score'].astype(float)
    pr = result['predicted_rating'].astype(float)

    ms_min, ms_max = ms.min(), ms.max()
    if ms_max > ms_min:
        ms_norm = (ms - ms_min) / (ms_max - ms_min)
    else:
        ms_norm = 0.0

    pr_min, pr_max = pr.min(), pr.max()
    if pr_max > pr_min:
        pr_norm = (pr - pr_min) / (pr_max - pr_min)
    else:
        pr_norm = 0.0

    # --- 3.5) Reweight: SVD is now dominant (25%) to boost precision ---
    alpha = 0.25  # rule-based importance (1 - alpha = 0.25 for SVD)

    if isinstance(ms_norm, (int, float)) and isinstance(pr_norm, (int, float)):
        result['final_score'] = alpha * ms_norm + (1 - alpha) * pr_norm
    else:
        result['final_score'] = alpha * ms_norm + (1 - alpha) * pr_norm

    # --- 4) Sort by final_score (best tutors first) ---
    result = result.sort_values('final_score', ascending=False)

    # --- 5) Drop duplicate tutors (keep best-scoring row per tutor_id) ---
    result = result.drop_duplicates(subset='tutor_id', keep='first')

    # --- 6) Select columns to show ---
    cols_to_show = [
        'tutor_id', 'tutor_name',
        'subject_name', 'course_name',
        'grade_level', 'education_type',
        'education_type_group' if 'education_type_group' in result.columns else None,
        'tutor_gender', 'teaches_gender',
        'session_type', 'group_or_private',
        'location_city',
        'handles_special_needs',
        'performance_focus',
        'experience_years',
        'average_rating_overall',
        'price_per_session',
        'match_score', 'predicted_rating', 'final_score'
    ]
    cols_to_show = [c for c in cols_to_show if c in result.columns]

    # --- 7) Apply top_n if requested ---
    if top_n is not None:
        return result[cols_to_show].head(top_n)
    else:
        return result[cols_to_show]

"""# Hybrid Recommendation Output

We generate hybrid recommendations for the first 5 students.
This confirms the system correctly ranks tutors using both rules and MF predictions.

Displays real examples of what a student would see.
"""

for sid in student_df['student_id'].unique()[:5]:
    print(f"\n============================")
    print(f"Hybrid recommendations for student {sid}")
    recs = recommend_tutors_hybrid(sid, top_n=5)
    if recs.empty:
        print("No recommendations found (this should not usually happen).")
    else:
        display(recs)

"""# Evaluation Metrics (Precision@5, Recall@5, NDCG@5)

To evaluate accuracy, we compare the predicted top-5 tutors with actual students‚Äô historical high ratings in the test set.

Metrics used:

* **"Precision@5: How many recommended tutors are actually relevant?"**

* **"Recall@5: How many relevant tutors were successfully retrieved?"**

* **"NDCG@5: Measures ranking quality (higher score if best tutors appear higher in list)"**

These metrics validate the model performance objectively.
"""

def precision_at_k_strict(K=5, min_rating=3.5):
    precisions = []

    for student_id in interaction_df['student_id'].unique():

        # Use your updated hybrid recommender here
        recs = recommend_tutors_hybrid(student_id, top_n=K)

        if recs is None or recs.empty:
            continue

        true_rel = interaction_df[
            (interaction_df['student_id'] == student_id) &
            (interaction_df['rating'] >= min_rating)
        ]['tutor_id'].unique().tolist()

        if len(true_rel) == 0:
            continue

        recommended_ids = recs['tutor_id'].tolist()
        hits = sum(1 for t in recommended_ids if t in true_rel)

        precisions.append(hits / K)

    if precisions:
        print(f"\nüìä HYBRID MODEL PRECISION@{K}: {np.mean(precisions):.4f}")
    else:
        print("No valid evaluation cases found.")

def precision_recall_ndcg_at_k(predictions, k=5, threshold=3.5):
    user_items = defaultdict(list)

    for uid, iid, true_r, est, _ in predictions:
        user_items[uid].append((est, true_r))

    precisions, recalls, ndcgs = [], [], []

    for uid, ratings in user_items.items():
        ratings.sort(key=lambda x: x[0], reverse=True)
        top_k = ratings[:k]

        total_rel = sum(true_r >= threshold for _, true_r in ratings)
        if total_rel == 0:
            continue

        hits = [(est >= threshold) and (true_r >= threshold) for est, true_r in top_k]

        # NEW: denominator = number of actually "recommended" items (pred >= threshold)
        num_rec = sum(est >= threshold for est, true_r in top_k)
        precision = (sum(hits) / num_rec) if num_rec > 0 else 0.0

        recall = sum(hits) / total_rel

        gains = [1 if h else 0 for h in hits]
        dcg = sum(g / math.log2(i + 2) for i, g in enumerate(gains))

        ideal_gains = [1] * min(k, total_rel)
        idcg = sum(g / math.log2(i + 2) for i, g in enumerate(ideal_gains))
        ndcg = dcg / idcg if idcg > 0 else 0.0

        precisions.append(precision)
        recalls.append(recall)
        ndcgs.append(ndcg)

    if not precisions:
        return 0.0, 0.0, 0.0

    return float(np.mean(precisions)), float(np.mean(recalls)), float(np.mean(ndcgs))

p5, r5, n5 = precision_recall_ndcg_at_k(predictions, k=5, threshold=3.5)

print("\nüìä STRICT METRICS (rating threshold ‚â• 3.5)")
print(f"Precision@5: {p5:.4f}")
print(f"Recall@5:    {r5:.4f}")
print(f"NDCG@5:      {n5:.4f}")

"""# Interpretation Summary

* **"High Recall (0.99) ‚Üí system successfully retrieves almost all good tutors"**

* **"High NDCG (0.95) ‚Üí relevant tutors appear near the top"**

* **"Moderate Precision (0.30) ‚Üí some non-ideal tutors are recommended too"**

This behavior indicates a recommender optimized for **broad discovery**, not strict filtering.

# Fairness Check (Popularity Bias)
A fair tutor recommender must not only suggest the most popular tutors.

This cell:

* **"Computes tutor popularity from number of ratings"**

* **"Defines top-20% as ‚Äúpopular tutors‚Äù"**

* **"Measures what percentage of recommendations:"**

* * **"Go to popular vs new/less-known tutors"**

* * **"Represent catalog coverage (how many different tutors appear)"**

Confirms whether recommendations give **fair exposure** to newer tutors.
"""

from collections import defaultdict
import numpy as np

# -------- 1) Rebuild preds from the SVD model (in case it's not in memory) --------
# K = how many tutors we look at in the ranking
K = 5

# preds: student_id -> list of (tutor_id, estimated_rating)
preds = defaultdict(list)

for uid, iid, true_r, est, _ in model.test(testset):
    preds[uid].append((iid, est))

# Sort each student's predictions by estimated rating (descending)
for uid in preds:
    preds[uid] = sorted(preds[uid], key=lambda x: x[1], reverse=True)

if len(preds) == 0:
    print("No predictions available from model.test(testset); cannot run fairness check.")
else:
    # -------- 2) Compute tutor popularity from interaction history --------
    if 'tutor_id' not in interaction_df.columns:
        raise ValueError("Column 'tutor_id' not found in interaction_df.")

    tutor_popularity = interaction_df['tutor_id'].value_counts()

    if tutor_popularity.empty:
        print("No tutor ratings available; cannot compute popularity-based fairness.")
    else:
        # Top 20% most-rated tutors = 'popular'
        popular_threshold = np.percentile(tutor_popularity.values, 80)
        popular_tutors = tutor_popularity[tutor_popularity >= popular_threshold].index.tolist()

        # -------- 3) Count exposure in recommendations --------
        exposure = defaultdict(int)
        total_recs = 0
        distinct_tutors_recommended = set()

        for uid in preds:
            top_k = [iid for iid, _ in preds[uid][:K]]
            if not top_k:
                continue

            total_recs += len(top_k)
            distinct_tutors_recommended.update(top_k)

            for tid in top_k:
                if tid in popular_tutors:
                    exposure["popular"] += 1
                else:
                    exposure["unpopular"] += 1

        if total_recs == 0:
            print("No recommendations generated; cannot compute fairness statistics.")
        else:
            popular_exposure_ratio = exposure["popular"] / total_recs
            coverage = len(distinct_tutors_recommended) / len(tutor_popularity)

            print("\n‚öñ Fairness Check Results:")
            print(f"Popular Tutor Exposure:   {popular_exposure_ratio*100:.2f}%")
            print(f"Unpopular Tutor Exposure: {(1-popular_exposure_ratio)*100:.2f}%")
            print(f"Catalog Coverage:         {coverage*100:.2f}% of all tutors")

"""Because our recommender retrieves many tutors (high recall), exposure is spread fairly across both popular and less-known profiles.
This prevents the system from becoming "biased toward only top-rated tutors", increasing opportunity and accessibility.
"""

!pip install gradio

import gradio as gr

def recommend_from_form(student_id,
                        subject_name,
                        grade_level,
                        education_type_group,
                        preferred_session_type,
                        preferred_session_group_type,
                        location_city,
                        special_needs,
                        performance_tier,
                        preferred_tutor_gender,
                        student_gender,
                        top_n):
    """
    Build a student profile from UI inputs, then:
    1) Use get_soft_candidates_for_student (rule-based matching)
    2) If SVD model exists, try to add predicted_rating
    3) Rank tutors and return a table
    """

    # 1Ô∏è‚É£ Build a Series-like "student" object
    s = pd.Series({
        'student_id': student_id,
        'subject_name': subject_name.strip().lower(),
        'grade_level': grade_level,
        'education_type_group': education_type_group,
        'preferred_session_type': preferred_session_type,
        'preferred_session_group_type': preferred_session_group_type,
        'location_city': location_city,
        'special_needs': special_needs,
        'performance_tier': performance_tier,
        'preferred_tutor_gender': preferred_tutor_gender,
        'gender': student_gender,   # used for teaches_gender rules
    })

    # 2Ô∏è‚É£ Rule-based candidate selection
    candidates = get_soft_candidates_for_student(s, tutors_df)

    if candidates is None or candidates.empty:
        return pd.DataFrame([{"message": "No tutors found for your preferences."}])

    # 3Ô∏è‚É£ Try to add predicted rating from SVD (if model is defined and knows this student)
    result = candidates.copy()

    use_mf = 'model' in globals() and model is not None
    scored = []

    if use_mf:
        try:
            for _, t in candidates.iterrows():
                tid = t['tutor_id']
                # For brand-new student IDs, Surprise may not know them; handle that
                try:
                    pred = model.predict(str(student_id), str(tid))
                    scored.append((tid, pred.est))
                except:
                    scored.append((tid, np.nan))

            scores_df = pd.DataFrame(scored, columns=['tutor_id', 'predicted_rating'])
            result = result.merge(scores_df, on='tutor_id', how='left')
        except:
            # If anything breaks, just skip MF
            result['predicted_rating'] = np.nan
    else:
        result['predicted_rating'] = np.nan

    # 4Ô∏è‚É£ Rank tutors: by match_score (and predicted_rating if available)
    # If predicted_rating is mostly NaN, sort will fall back to match_score only
    if 'predicted_rating' in result.columns:
        result = result.sort_values(['match_score', 'predicted_rating'],
                                    ascending=False, na_position='last')
    else:
        result = result.sort_values('match_score', ascending=False)

    # Remove duplicate tutors (keep best row)
    result = result.drop_duplicates(subset='tutor_id', keep='first')

    # 5Ô∏è‚É£ Select columns to show in the table
    cols_to_show = [
        'tutor_id', 'tutor_name',
        'subject_name', 'course_name',
        'grade_level', 'education_type', 'education_type_group',
        'tutor_gender', 'teaches_gender',
        'session_type', 'group_or_private',
        'location_city',
        'handles_special_needs',
        'performance_focus',
        'experience_years',
        'average_rating_overall',
        'price_per_session',
        'match_score', 'predicted_rating'
    ]
    cols_to_show = [c for c in cols_to_show if c in result.columns]

    # 6Ô∏è‚É£ Limit number of tutors shown
    top_n = int(top_n)
    return result[cols_to_show].head(top_n).reset_index(drop=True)

# Prepare dropdown choices from your data (for better UX)
subject_choices = sorted(tutors_df['subject_name'].dropna().unique().tolist())
edu_group_choices = sorted(tutors_df['education_type_group'].dropna().unique().tolist())
session_choices = sorted(tutors_df['session_type'].dropna().unique().tolist()) \
                  if 'session_type' in tutors_df.columns else ["online", "face-to-face", "hybrid"]
group_choices = sorted(tutors_df['group_or_private'].dropna().unique().tolist()) \
                if 'group_or_private' in tutors_df.columns else ["individual", "group", "both"]
city_choices = sorted(tutors_df['location_city'].dropna().unique().tolist())
special_needs_choices = ["none", "adhd", "dyslexia", "autism", "hearing impairment"]
perf_choices = ["Beginner", "Intermediate", "Advanced"]
gender_choices = ["male", "female"]
tutor_gender_pref_choices = ["Any", "Male", "Female"]

with gr.Blocks() as demo:
    gr.Markdown("## üéì Smart Tutor Recommendation ‚Äì Student Portal")
    gr.Markdown(
        "Enter your details and we‚Äôll recommend the best matching tutors "
        "based on subject, curriculum, preferences, and our ML model."
    )

    with gr.Row():
        student_id_input = gr.Textbox(label="Student ID", placeholder="e.g., S12345")

    with gr.Row():
        subject_input = gr.Dropdown(
            choices=subject_choices,
            label="What subject are you struggling with?",
            info="Choose the main subject (e.g., math, physics, chemistry)"
        )
        grade_input = gr.Number(
            label="Your Grade Level",
            value=10,
            precision=0
        )

    with gr.Row():
        edu_group_input = gr.Dropdown(
            choices=edu_group_choices,
            label="Your Curriculum Type",
            info="e.g., Saudi/MOE, British/IGCSE/A-level, American, IB, Desi..."
        )
        perf_input = gr.Dropdown(
            choices=perf_choices,
            label="Your Performance Level",
            value="Intermediate"
        )

    with gr.Row():
        session_type_input = gr.Dropdown(
            choices=session_choices,
            label="Preferred Session Type",
            value=session_choices[0] if session_choices else "online"
        )
        group_type_input = gr.Dropdown(
            choices=group_choices,
            label="Preferred Session Style",
            value=group_choices[0] if group_choices else "individual"
        )

    with gr.Row():
        city_input = gr.Dropdown(
            choices=city_choices,
            label="Your City (for face-to-face/hybrid)"
        )
        special_needs_input = gr.Dropdown(
            choices=special_needs_choices,
            label="Special Needs (if any)",
            value="none"
        )

    with gr.Row():
        student_gender_input = gr.Dropdown(
            choices=gender_choices,
            label="Your Gender",
            value="female"
        )
        tutor_gender_pref_input = gr.Dropdown(
            choices=tutor_gender_pref_choices,
            label="Preferred Tutor Gender",
            value="Any"
        )

    topn_input = gr.Slider(
        minimum=3,
        maximum=20,
        value=5,
        step=1,
        label="How many tutors should we recommend?"
    )

    recommend_button = gr.Button("üîç Find My Tutors")

    output_table = gr.Dataframe(
        label="Recommended Tutors",
        interactive=False
    )

    recommend_button.click(
        fn=recommend_from_form,
        inputs=[
            student_id_input,
            subject_input,
            grade_input,
            edu_group_input,
            session_type_input,
            group_type_input,
            city_input,
            special_needs_input,
            perf_input,
            tutor_gender_pref_input,
            student_gender_input,
            topn_input
        ],
        outputs=output_table
    )

demo.launch()